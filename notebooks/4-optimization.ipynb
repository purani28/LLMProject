{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ea6d56d94c4990963358186f719eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6eaa45b39904fe5ad332f72d15ec9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b94343c0914b079b1f37864a042ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krish\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_4428\\3337468621.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545ea6f4d1c345bc95ed976dd2eaedd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9471, 'grad_norm': 6.38163423538208, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.2}\n",
      "{'loss': 0.7367, 'grad_norm': 8.004007339477539, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.4}\n",
      "{'loss': 0.6834, 'grad_norm': 4.16328239440918, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.6}\n",
      "{'loss': 0.6168, 'grad_norm': 6.591975212097168, 'learning_rate': 1.4666666666666666e-05, 'epoch': 0.8}\n",
      "{'loss': 0.5141, 'grad_norm': 12.136236190795898, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a4bbdc71c04a438bf31c303c3e4e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4423254430294037, 'eval_accuracy': 0.82, 'eval_f1': 0.81998199819982, 'eval_runtime': 105.2845, 'eval_samples_per_second': 1.9, 'eval_steps_per_second': 0.123, 'epoch': 1.0}\n",
      "{'loss': 0.4265, 'grad_norm': 31.69100570678711, 'learning_rate': 1.2e-05, 'epoch': 1.2}\n",
      "{'loss': 0.3714, 'grad_norm': 16.750341415405273, 'learning_rate': 1.0666666666666667e-05, 'epoch': 1.4}\n",
      "{'loss': 0.3483, 'grad_norm': 15.036824226379395, 'learning_rate': 9.333333333333334e-06, 'epoch': 1.6}\n",
      "{'loss': 0.4174, 'grad_norm': 38.17802429199219, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.8}\n",
      "{'loss': 0.3066, 'grad_norm': 19.477554321289062, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc674deb1cb24c0d844e431d52bc98d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.39956527948379517, 'eval_accuracy': 0.845, 'eval_f1': 0.8448098921178444, 'eval_runtime': 111.2702, 'eval_samples_per_second': 1.797, 'eval_steps_per_second': 0.117, 'epoch': 2.0}\n",
      "{'loss': 0.337, 'grad_norm': 23.309993743896484, 'learning_rate': 5.333333333333334e-06, 'epoch': 2.2}\n",
      "{'loss': 0.2702, 'grad_norm': 18.441246032714844, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.4}\n",
      "{'loss': 0.2407, 'grad_norm': 1.7277811765670776, 'learning_rate': 2.666666666666667e-06, 'epoch': 2.6}\n",
      "{'loss': 0.2597, 'grad_norm': 54.012001037597656, 'learning_rate': 1.3333333333333334e-06, 'epoch': 2.8}\n",
      "{'loss': 0.1626, 'grad_norm': 17.924144744873047, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4eec655c34e482ca3141f36399dab38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.466318815946579, 'eval_accuracy': 0.855, 'eval_f1': 0.8545600441335038, 'eval_runtime': 61.2469, 'eval_samples_per_second': 3.265, 'eval_steps_per_second': 0.212, 'epoch': 3.0}\n",
      "{'train_runtime': 5042.9065, 'train_samples_per_second': 0.476, 'train_steps_per_second': 0.03, 'train_loss': 0.44256450653076174, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb1b9d85be8440d84ff7bae068bba28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad0cb371ff0489994a2e3e884e805c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86       103\n",
      "           1       0.87      0.82      0.85        97\n",
      "\n",
      "    accuracy                           0.85       200\n",
      "   macro avg       0.86      0.85      0.85       200\n",
      "weighted avg       0.86      0.85      0.85       200\n",
      "\n",
      "Confusion Matrix:\n",
      " [[91 12]\n",
      " [17 80]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import Trainer, TrainingArguments, RobertaTokenizer, RobertaForSequenceClassification\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the cleaned dataset from 'cleaned_dataset.csv'\n",
    "df = pd.read_csv('cleaned_reviews.csv')\n",
    "\n",
    "# Create HuggingFace Dataset from the pandas DataFrame\n",
    "dataset = Dataset.from_pandas(df[['ProcessedText', 'Sentiment']])\n",
    "\n",
    "# Label encoding: Convert 'positive' to 0 and 'negative' to 1\n",
    "label_encoder = {'positive': 0, 'negative': 1}\n",
    "dataset = dataset.map(lambda x: {'labels': label_encoder[x['Sentiment']]}, remove_columns=['Sentiment'])\n",
    "\n",
    "# Split the dataset into train and validation\n",
    "train_ds, eval_ds = dataset.train_test_split(test_size=0.2).values()\n",
    "\n",
    "# Load a tokenizer and model (Roberta for sequence classification)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n",
    "\n",
    "# Tokenize datasets\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['ProcessedText'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "train_ds = train_ds.map(tokenize_function, batched=True)\n",
    "eval_ds = eval_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "# TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',           \n",
    "    evaluation_strategy=\"epoch\",      \n",
    "    learning_rate=2e-5,               \n",
    "    per_device_train_batch_size=16,   \n",
    "    per_device_eval_batch_size=16,    \n",
    "    num_train_epochs=3,               \n",
    "    weight_decay=0.01,                \n",
    "    push_to_hub=False,                \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=10,                 \n",
    ")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(axis=-1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(p.label_ids, preds),\n",
    "        'f1': classification_report(p.label_ids, preds, output_dict=True)['macro avg']['f1-score']\n",
    "    }\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate model \n",
    "trainer.evaluate()\n",
    "\n",
    "predictions, labels, _ = trainer.predict(eval_ds)\n",
    "predicted_labels = predictions.argmax(axis=-1)\n",
    "\n",
    "# classification report and confusion matrix\n",
    "print(\"Classification Report:\\n\", classification_report(labels, predicted_labels))\n",
    "cm = confusion_matrix(labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model results file was too long so i have them compressed into a zip folder on my google drive. Here's the link to access them : https://drive.google.com/file/d/1yofFDytjDqGvvfDX3BQD3xJ3X9ZK6SDI/view?usp=sharing  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
